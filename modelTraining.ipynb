{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score,\\\n",
    "recall_score, confusion_matrix, classification_report, accuracy_score \n",
    "import nltk\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __GetLIWC(file:str): \n",
    "\tliwc = pd.read_csv(file)\n",
    "\tliwc = liwc.rename(columns = {liwc.columns[2]:'user_id'})\n",
    "\tliwcUser = liwc.groupby('user_id').mean().reset_index()\n",
    "\tliwcUser = liwcUser.drop(['Source (A)', 'Source (D)'], axis=1)\n",
    "\treturn liwcUser\n",
    "\n",
    "def mergeFea(features, liwc, empath): \n",
    "\tfeatures = pd.read_csv(features)\n",
    "\n",
    "\t#merge features\n",
    "\tliwcUser = __GetLIWC(liwc)\n",
    "\tliwcUser2 = liwcUser.iloc[:,1::]\n",
    "\tliwcUser2.columns = [str(col) + '_liwc' for col in liwcUser2.columns]\n",
    "\tliwcUser2['user_id'] = liwcUser.user_id\n",
    "\n",
    "\tempath = pd.read_csv(empath)\n",
    "\tempath2 = empath.iloc[:,1::]\n",
    "\tempath2.columns = [str(col) + '_empath' for col in empath2.columns]\n",
    "\tempath2['user_id'] = empath.user_id\n",
    "\n",
    "\tallfea = pd.merge(features, liwcUser2, on = 'user_id', how = 'right')\n",
    "\tallfea = pd.merge(allfea, empath2, on = 'user_id', how = 'right')\n",
    "\treturn allfea\n",
    "\n",
    "def preprocess2(sent):\n",
    "    #remove punctustion\n",
    "    sent = re.sub(r'[^\\w\\s]','',str(sent))\n",
    "    words = sent.split()\n",
    "    new_words = []\n",
    "    for w in words:      \n",
    "        new_words.append(w.lower())\n",
    "        \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key].values.reshape(-1,1)\n",
    "    \n",
    "class ItemSelectorText(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [0],\n",
       "       [4],\n",
       "       [1],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [0],\n",
       "       [3],\n",
       "       [5],\n",
       "       [2],\n",
       "       [5],\n",
       "       [1],\n",
       "       [1],\n",
       "       [4],\n",
       "       [5],\n",
       "       [2],\n",
       "       [0],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [0],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [1],\n",
       "       [4],\n",
       "       [5],\n",
       "       [2],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [5],\n",
       "       [1],\n",
       "       [4],\n",
       "       [4],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [2],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [4],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [2],\n",
       "       [4],\n",
       "       [5],\n",
       "       [3],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [3],\n",
       "       [2],\n",
       "       [2],\n",
       "       [4],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [2],\n",
       "       [4],\n",
       "       [2],\n",
       "       [4],\n",
       "       [2],\n",
       "       [4],\n",
       "       [5],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [2],\n",
       "       [3],\n",
       "       [2],\n",
       "       [4],\n",
       "       [5],\n",
       "       [2],\n",
       "       [1],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [4],\n",
       "       [1],\n",
       "       [5],\n",
       "       [4],\n",
       "       [0],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [3],\n",
       "       [3],\n",
       "       [1],\n",
       "       [5],\n",
       "       [2],\n",
       "       [1],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [1],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [3],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [1],\n",
       "       [3],\n",
       "       [5],\n",
       "       [4],\n",
       "       [1],\n",
       "       [3],\n",
       "       [2],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [5],\n",
       "       [2],\n",
       "       [3],\n",
       "       [1],\n",
       "       [1],\n",
       "       [4],\n",
       "       [2],\n",
       "       [4],\n",
       "       [4],\n",
       "       [3],\n",
       "       [2],\n",
       "       [5],\n",
       "       [3],\n",
       "       [0],\n",
       "       [3],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [4],\n",
       "       [5],\n",
       "       [2],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [3],\n",
       "       [3],\n",
       "       [5],\n",
       "       [1],\n",
       "       [5],\n",
       "       [1],\n",
       "       [0],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [3],\n",
       "       [2],\n",
       "       [1],\n",
       "       [5],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [0],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [0],\n",
       "       [1],\n",
       "       [5],\n",
       "       [3],\n",
       "       [3],\n",
       "       [4],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [0],\n",
       "       [1],\n",
       "       [5],\n",
       "       [4],\n",
       "       [3],\n",
       "       [2],\n",
       "       [0],\n",
       "       [5],\n",
       "       [3],\n",
       "       [2],\n",
       "       [5],\n",
       "       [3],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [5],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [1],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [3],\n",
       "       [4],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [3],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [0],\n",
       "       [5],\n",
       "       [5],\n",
       "       [0],\n",
       "       [5],\n",
       "       [4],\n",
       "       [4],\n",
       "       [2],\n",
       "       [5],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [1],\n",
       "       [3],\n",
       "       [5],\n",
       "       [4],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [0],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [0],\n",
       "       [2],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [5],\n",
       "       [1],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [5],\n",
       "       [0],\n",
       "       [4],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [1],\n",
       "       [5],\n",
       "       [4],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [2]])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = ItemSelector('motivations')\n",
    "obj.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = '/home/lucia/phd_work/shareTask/'\n",
    "path = '/Users/lucia/phd_work/Clpsy/'\n",
    "X = pd.read_csv(path + '/data/clpsych19_training_data/Btrain_NoNoise_SW.csv')\n",
    "y = pd.read_csv(path + '/data/clpsych19_training_data/crowd_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate text according to user id\n",
    "X1 = X[['user_id','post_body']]\n",
    "conTex = X1.groupby(['user_id'],as_index=False).agg(lambda x : x.sum() if str(x) else ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conTex['post_body'] = conTex['post_body'].apply(lambda x: preprocess2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'user_id', 'raw_label', 'postingFrequency',\n",
       "       'postingInterval', 'generalMoreFreq', 'generalWordCount',\n",
       "       'healthPostingFrequency', 'healthPostingInterval', 'healthMoreFreq',\n",
       "       'healthWordCount', 'mentionMethods', 'SWFrequency', 'SWPostingInterval',\n",
       "       'SWFreq', 'SWWordCount', 'fin_body', 'drug_body', 'mental_body',\n",
       "       'rela_body', 'suicide_body', 'hopeless_body', 'motivations',\n",
       "       'family_senti', 'partner_senti', 'self_senti', 'raw_label.1',\n",
       "       'sentiment', 'mclust'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fea = pd.read_csv(path + '/suicideDetection/features/FreqSentiMotiTopiFea.csv')\n",
    "Fea.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFea = pd.read_csv(path + '/suicideDetection/features/FreqSentiMotiTopiFea.csv')\n",
    "liwc = pd.read_csv(path + '/suicideDetection/features/liwcSW.csv')\n",
    "tags = pd.read_csv(path + '/suicideDetection/features/TagFeaSW.csv')\n",
    "empath = pd.read_csv(path + '/suicideDetection/features/empathSW.csv')\n",
    "readability = pd.read_csv(path + '/suicideDetection/features/readability.csv')\n",
    "\n",
    "liwc = liwc.iloc[:,np.r_[2, 8:liwc.shape[1]]]\n",
    "liwc = liwc.rename(columns = {liwc.columns[0]:'user_id'})\n",
    "liwcUser = liwc.groupby('user_id').mean().reset_index()\n",
    "liwcUser.columns = [str(col) + '_liwc' for col in liwcUser.columns]\n",
    "readability = readability.drop(['post_body'], axis = 1)\n",
    "\n",
    "empath.columns = [str(col) + '_empath' for col in empath.columns]\n",
    "tags.columns = [str(col) + '_tag' for col in tags.columns]\n",
    "readability.columns = [str(col) + '_read' for col in readability.columns]\n",
    "allFea = pd.merge(allFea, liwcUser, left_on ='user_id', right_on = 'user_id_liwc', how = 'left')\n",
    "allFea = pd.merge(allFea, empath, left_on ='user_id', right_on = 'user_id_empath', how = 'left')\n",
    "allFea = pd.merge(allFea, tags, left_on ='user_id', right_on = 'user_id_tag', how = 'left')\n",
    "allFea = pd.merge(allFea, readability, left_on ='user_id', right_on = 'user_id_read', how = 'left')\n",
    "\n",
    "# # # #liwc\n",
    "# # # allFea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fea = pd.merge(y, conTex, on = 'user_id')\n",
    "allFea2  = pd.merge(conTex, allFea, on = 'user_id')\n",
    "#X = fea['post_body']\n",
    "y = allFea2.raw_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(allFea2, y, test_size=0.30, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(347, 183)\n",
      "(347,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'post_body', 'Unnamed: 0', 'raw_label', 'postingFrequency',\n",
       "       'postingInterval', 'generalMoreFreq', 'generalWordCount',\n",
       "       'healthPostingFrequency', 'healthPostingInterval',\n",
       "       ...\n",
       "       'VBN_tag', 'VBZ_tag', 'Unnamed: 0_read', 'user_id_read',\n",
       "       'readEase_read', 'Flesch-Kincaid_read', 'gunning_fog_read',\n",
       "       'smog_index_read', 'coleman_liau_read', 'linsear_write_read'],\n",
       "      dtype='object', length=183)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           a       0.75      0.69      0.72        35\n",
      "           b       0.14      0.06      0.08        18\n",
      "           c       0.29      0.25      0.27        32\n",
      "           d       0.59      0.75      0.66        64\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       149\n",
      "   macro avg       0.44      0.44      0.43       149\n",
      "weighted avg       0.51      0.54      0.52       149\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucia/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/lucia/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lucia/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "get_liwc_data = FunctionTransformer(lambda x: x[x.columns[x.columns.to_series().str.contains('liwc')]], validate=False)\n",
    "get_empath_data = FunctionTransformer(lambda x: x[x.columns[x.columns.to_series().str.contains('empath')]], validate=False)\n",
    "get_tags = FunctionTransformer(lambda x: x[x.columns[x.columns.to_series().str.contains('tag')]], validate=False)\n",
    "get_read = FunctionTransformer(lambda x: x[x.columns[x.columns.to_series().str.contains('read')]], validate=False)\n",
    "\n",
    "\n",
    "pipe1 = Pipeline([\n",
    "    \n",
    "    ('feats', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('selector', ItemSelectorText(key='post_body')),\n",
    "            ('cv', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "             ])),\n",
    "        ('liwc_features', Pipeline([\n",
    "                ('selector', get_liwc_data) ])),\n",
    "        ('empath_features', Pipeline([\n",
    "                ('selector', get_empath_data)])),\n",
    "        ('tag_features', Pipeline([\n",
    "                ('selector', get_tags)])),\n",
    "#         ('readability_features', Pipeline([\n",
    "#                 ('selector', get_read)])),\n",
    "        ('selector1', ItemSelector(key='motivations')),\n",
    "#         ('selector2', ItemSelector(key='healthPostingFrequency')),\n",
    "#         ('selector3', ItemSelector(key='healthPostingInterval')),\n",
    "#         ('selector4', ItemSelector(key='healthMoreFreq')),\n",
    "        ('selector5', ItemSelector(key='healthWordCount')),\n",
    "        ('selector6', ItemSelector(key='mentionMethods')),\n",
    "        ('selector7', ItemSelector(key='SWFrequency')),\n",
    "#         ('selector8', ItemSelector(key='SWPostingInterval')),\n",
    "#         ('selector9', ItemSelector(key='SWFreq')),\n",
    "#         ('selector10', ItemSelector(key='suicide_body')),\n",
    "#         ('selector11', ItemSelector(key='hopeless_body')),\n",
    "#         ('selector12', ItemSelector(key='self_senti')),\n",
    "#         ('selector13', ItemSelector(key='mclust')),\n",
    "#         ('selector14', ItemSelector(key='family_senti')),\n",
    "             ])),\n",
    "    \n",
    "       ('clf', Pipeline([\n",
    "       ('scale', StandardScaler(with_mean=False)),\n",
    "       ('feature_selection', SelectFromModel(RandomForestClassifier(random_state=20))),\n",
    "#       ('svm',  svm.SVC())\n",
    "       ('log', LogisticRegression()),\n",
    "#      ('gbn', GaussianNB()),\n",
    "         ])),\n",
    "])\n",
    "\n",
    "# parameters = [{'feature_selection__estimator__max_depth': [5,10,20], 'feature_selection__estimator__max_leaf_nodes': [50, 100, 200],\n",
    "#             'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'svc__gamma': [0.01, 0.001, 0.0001],\n",
    "#             'svc__C':[0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.5, 2.0, 10] , 'svc__class_weight':['balanced']}]\n",
    "parameters = [{'feature_selection__estimator__max_depth': [5,10,20], 'feature_selection__estimator__max_leaf_nodes': [50, 100, 200],\n",
    "               'log__C':[1.0, 2.0, 3.0], 'clf__log__class_weight': ['balanced'], 'multi_class': ['ovr', 'multinomial']}]\n",
    "    \n",
    "    \n",
    "estimator = GridSearchCV(pipe1, parameters, scoring='roc_auc')\n",
    "pipe1.fit(X_train, y_train)\n",
    "pred = pipe1.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "รท"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           a       0.70      0.66      0.68        35\n",
      "           b       0.33      0.11      0.17        18\n",
      "           c       0.19      0.22      0.20        32\n",
      "           d       0.55      0.62      0.58        64\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       149\n",
      "   macro avg       0.44      0.40      0.41       149\n",
      "weighted avg       0.48      0.48      0.47       149\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucia/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/lucia/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lucia/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pipe1.fit(X_train, y_train)\n",
    "pred = pipe1.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feats', FeatureUnion(n_jobs=None,\n",
       "         transformer_list=[('text', Pipeline(memory=None,\n",
       "       steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_f...body')), ('selector12', ItemSelector(key='self_senti')), ('selector13', ItemSelector(key='mclust'))],\n",
       "         transformer_weights=None)), ('clf', Pipeline(memory=None,\n",
       "       steps=[('scale', StandardScaler(copy=True, with_mean=False, with_std=True)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False))]))]"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           a       0.69      0.69      0.69        35\n",
      "           b       0.20      0.06      0.09        18\n",
      "           c       0.23      0.19      0.21        32\n",
      "           d       0.55      0.72      0.63        64\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       149\n",
      "   macro avg       0.42      0.41      0.40       149\n",
      "weighted avg       0.47      0.52      0.48       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = pipe1.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check parameter names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'logit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-307-61c432e49ad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'logit'"
     ]
    }
   ],
   "source": [
    "pipe1.named_steps['logit'].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': Pipeline(memory=None,\n",
       "      steps=[('scale', StandardScaler(copy=True, with_mean=False, with_std=True)), ('feature_selection', SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decr...penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False))]),\n",
       " 'clf__feature_selection': SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "             oob_score=False, random_state=20, verbose=0, warm_start=False),\n",
       "         max_features=None, norm_order=1, prefit=False, threshold=None),\n",
       " 'clf__feature_selection__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "             oob_score=False, random_state=20, verbose=0, warm_start=False),\n",
       " 'clf__feature_selection__estimator__bootstrap': True,\n",
       " 'clf__feature_selection__estimator__class_weight': None,\n",
       " 'clf__feature_selection__estimator__criterion': 'gini',\n",
       " 'clf__feature_selection__estimator__max_depth': None,\n",
       " 'clf__feature_selection__estimator__max_features': 'auto',\n",
       " 'clf__feature_selection__estimator__max_leaf_nodes': None,\n",
       " 'clf__feature_selection__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__feature_selection__estimator__min_impurity_split': None,\n",
       " 'clf__feature_selection__estimator__min_samples_leaf': 1,\n",
       " 'clf__feature_selection__estimator__min_samples_split': 2,\n",
       " 'clf__feature_selection__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__feature_selection__estimator__n_estimators': 'warn',\n",
       " 'clf__feature_selection__estimator__n_jobs': None,\n",
       " 'clf__feature_selection__estimator__oob_score': False,\n",
       " 'clf__feature_selection__estimator__random_state': 20,\n",
       " 'clf__feature_selection__estimator__verbose': 0,\n",
       " 'clf__feature_selection__estimator__warm_start': False,\n",
       " 'clf__feature_selection__max_features': None,\n",
       " 'clf__feature_selection__norm_order': 1,\n",
       " 'clf__feature_selection__prefit': False,\n",
       " 'clf__feature_selection__threshold': None,\n",
       " 'clf__log': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       " 'clf__log__C': 1.0,\n",
       " 'clf__log__class_weight': None,\n",
       " 'clf__log__dual': False,\n",
       " 'clf__log__fit_intercept': True,\n",
       " 'clf__log__intercept_scaling': 1,\n",
       " 'clf__log__max_iter': 100,\n",
       " 'clf__log__multi_class': 'warn',\n",
       " 'clf__log__n_jobs': None,\n",
       " 'clf__log__penalty': 'l2',\n",
       " 'clf__log__random_state': None,\n",
       " 'clf__log__solver': 'warn',\n",
       " 'clf__log__tol': 0.0001,\n",
       " 'clf__log__verbose': 0,\n",
       " 'clf__log__warm_start': False,\n",
       " 'clf__memory': None,\n",
       " 'clf__scale': StandardScaler(copy=True, with_mean=False, with_std=True),\n",
       " 'clf__scale__copy': True,\n",
       " 'clf__scale__with_mean': False,\n",
       " 'clf__scale__with_std': True,\n",
       " 'clf__steps': [('scale',\n",
       "   StandardScaler(copy=True, with_mean=False, with_std=True)),\n",
       "  ('feature_selection',\n",
       "   SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "               oob_score=False, random_state=20, verbose=0, warm_start=False),\n",
       "           max_features=None, norm_order=1, prefit=False, threshold=None)),\n",
       "  ('log',\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False))],\n",
       " 'feats': FeatureUnion(n_jobs=None,\n",
       "        transformer_list=[('text', Pipeline(memory=None,\n",
       "      steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_f...body')), ('selector12', ItemSelector(key='self_senti')), ('selector13', ItemSelector(key='mclust'))],\n",
       "        transformer_weights=None),\n",
       " 'feats__empath_features': Pipeline(memory=None,\n",
       "      steps=[('selector', FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "           func=<function <lambda> at 0x1a18df50d0>, inv_kw_args=None,\n",
       "           inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "           validate=False))]),\n",
       " 'feats__empath_features__memory': None,\n",
       " 'feats__empath_features__selector': FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "           func=<function <lambda> at 0x1a18df50d0>, inv_kw_args=None,\n",
       "           inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "           validate=False),\n",
       " 'feats__empath_features__selector__accept_sparse': False,\n",
       " 'feats__empath_features__selector__check_inverse': True,\n",
       " 'feats__empath_features__selector__func': <function __main__.<lambda>>,\n",
       " 'feats__empath_features__selector__inv_kw_args': None,\n",
       " 'feats__empath_features__selector__inverse_func': None,\n",
       " 'feats__empath_features__selector__kw_args': None,\n",
       " 'feats__empath_features__selector__pass_y': 'deprecated',\n",
       " 'feats__empath_features__selector__validate': False,\n",
       " 'feats__empath_features__steps': [('selector',\n",
       "   FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "             func=<function <lambda> at 0x1a18df50d0>, inv_kw_args=None,\n",
       "             inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "             validate=False))],\n",
       " 'feats__liwc_features': Pipeline(memory=None,\n",
       "      steps=[('selector', FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "           func=<function <lambda> at 0x1a18df5048>, inv_kw_args=None,\n",
       "           inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "           validate=False))]),\n",
       " 'feats__liwc_features__memory': None,\n",
       " 'feats__liwc_features__selector': FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "           func=<function <lambda> at 0x1a18df5048>, inv_kw_args=None,\n",
       "           inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "           validate=False),\n",
       " 'feats__liwc_features__selector__accept_sparse': False,\n",
       " 'feats__liwc_features__selector__check_inverse': True,\n",
       " 'feats__liwc_features__selector__func': <function __main__.<lambda>>,\n",
       " 'feats__liwc_features__selector__inv_kw_args': None,\n",
       " 'feats__liwc_features__selector__inverse_func': None,\n",
       " 'feats__liwc_features__selector__kw_args': None,\n",
       " 'feats__liwc_features__selector__pass_y': 'deprecated',\n",
       " 'feats__liwc_features__selector__validate': False,\n",
       " 'feats__liwc_features__steps': [('selector',\n",
       "   FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "             func=<function <lambda> at 0x1a18df5048>, inv_kw_args=None,\n",
       "             inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "             validate=False))],\n",
       " 'feats__n_jobs': None,\n",
       " 'feats__selector1': ItemSelector(key='motivations'),\n",
       " 'feats__selector10': ItemSelector(key='suicide_body'),\n",
       " 'feats__selector10__key': 'suicide_body',\n",
       " 'feats__selector11': ItemSelector(key='hopeless_body'),\n",
       " 'feats__selector11__key': 'hopeless_body',\n",
       " 'feats__selector12': ItemSelector(key='self_senti'),\n",
       " 'feats__selector12__key': 'self_senti',\n",
       " 'feats__selector13': ItemSelector(key='mclust'),\n",
       " 'feats__selector13__key': 'mclust',\n",
       " 'feats__selector1__key': 'motivations',\n",
       " 'feats__selector2': ItemSelector(key='healthPostingFrequency'),\n",
       " 'feats__selector2__key': 'healthPostingFrequency',\n",
       " 'feats__selector3': ItemSelector(key='healthPostingInterval'),\n",
       " 'feats__selector3__key': 'healthPostingInterval',\n",
       " 'feats__selector4': ItemSelector(key='healthMoreFreq'),\n",
       " 'feats__selector4__key': 'healthMoreFreq',\n",
       " 'feats__selector5': ItemSelector(key='healthWordCount'),\n",
       " 'feats__selector5__key': 'healthWordCount',\n",
       " 'feats__selector6': ItemSelector(key='mentionMethods'),\n",
       " 'feats__selector6__key': 'mentionMethods',\n",
       " 'feats__selector7': ItemSelector(key='SWFrequency'),\n",
       " 'feats__selector7__key': 'SWFrequency',\n",
       " 'feats__selector8': ItemSelector(key='SWPostingInterval'),\n",
       " 'feats__selector8__key': 'SWPostingInterval',\n",
       " 'feats__selector9': ItemSelector(key='SWFreq'),\n",
       " 'feats__selector9__key': 'SWFreq',\n",
       " 'feats__text': Pipeline(memory=None,\n",
       "      steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), pr...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))]),\n",
       " 'feats__text__cv': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'feats__text__cv__analyzer': 'word',\n",
       " 'feats__text__cv__binary': False,\n",
       " 'feats__text__cv__decode_error': 'strict',\n",
       " 'feats__text__cv__dtype': numpy.int64,\n",
       " 'feats__text__cv__encoding': 'utf-8',\n",
       " 'feats__text__cv__input': 'content',\n",
       " 'feats__text__cv__lowercase': True,\n",
       " 'feats__text__cv__max_df': 1.0,\n",
       " 'feats__text__cv__max_features': None,\n",
       " 'feats__text__cv__min_df': 1,\n",
       " 'feats__text__cv__ngram_range': (1, 1),\n",
       " 'feats__text__cv__preprocessor': None,\n",
       " 'feats__text__cv__stop_words': None,\n",
       " 'feats__text__cv__strip_accents': None,\n",
       " 'feats__text__cv__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'feats__text__cv__tokenizer': None,\n",
       " 'feats__text__cv__vocabulary': None,\n",
       " 'feats__text__memory': None,\n",
       " 'feats__text__selector': ItemSelectorText(key='post_body'),\n",
       " 'feats__text__selector__key': 'post_body',\n",
       " 'feats__text__steps': [('selector', ItemSelectorText(key='post_body')),\n",
       "  ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))],\n",
       " 'feats__text__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'feats__text__tfidf__norm': 'l2',\n",
       " 'feats__text__tfidf__smooth_idf': True,\n",
       " 'feats__text__tfidf__sublinear_tf': False,\n",
       " 'feats__text__tfidf__use_idf': True,\n",
       " 'feats__transformer_list': [('text', Pipeline(memory=None,\n",
       "        steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), pr...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))])),\n",
       "  ('liwc_features', Pipeline(memory=None,\n",
       "        steps=[('selector', FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "             func=<function <lambda> at 0x1a18df5048>, inv_kw_args=None,\n",
       "             inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "             validate=False))])),\n",
       "  ('empath_features', Pipeline(memory=None,\n",
       "        steps=[('selector', FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "             func=<function <lambda> at 0x1a18df50d0>, inv_kw_args=None,\n",
       "             inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "             validate=False))])),\n",
       "  ('selector1', ItemSelector(key='motivations')),\n",
       "  ('selector2', ItemSelector(key='healthPostingFrequency')),\n",
       "  ('selector3', ItemSelector(key='healthPostingInterval')),\n",
       "  ('selector4', ItemSelector(key='healthMoreFreq')),\n",
       "  ('selector5', ItemSelector(key='healthWordCount')),\n",
       "  ('selector6', ItemSelector(key='mentionMethods')),\n",
       "  ('selector7', ItemSelector(key='SWFrequency')),\n",
       "  ('selector8', ItemSelector(key='SWPostingInterval')),\n",
       "  ('selector9', ItemSelector(key='SWFreq')),\n",
       "  ('selector10', ItemSelector(key='suicide_body')),\n",
       "  ('selector11', ItemSelector(key='hopeless_body')),\n",
       "  ('selector12', ItemSelector(key='self_senti')),\n",
       "  ('selector13', ItemSelector(key='mclust'))],\n",
       " 'feats__transformer_weights': None,\n",
       " 'memory': None,\n",
       " 'steps': [('feats', FeatureUnion(n_jobs=None,\n",
       "          transformer_list=[('text', Pipeline(memory=None,\n",
       "        steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_f...body')), ('selector12', ItemSelector(key='self_senti')), ('selector13', ItemSelector(key='mclust'))],\n",
       "          transformer_weights=None)), ('clf', Pipeline(memory=None,\n",
       "        steps=[('scale', StandardScaler(copy=True, with_mean=False, with_std=True)), ('feature_selection', SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decr...penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False))]))]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter cv for estimator Pipeline(memory=None,\n     steps=[('feats', FeatureUnion(n_jobs=None,\n       transformer_list=[('text', Pipeline(memory=None,\n     steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='c...alty='l2', random_state=None, solver='warn',\n          tol=0.0001, verbose=0, warm_start=False))]))]). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-309-620378fa36af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m pipe1.set_params(cv__min_df=6, \n\u001b[0;32m----> 2\u001b[0;31m                  cv__lowercase=False).fit(X_train, y_train)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m_set_params\u001b[0;34m(self, attr, **params)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# 3. Step parameters and other initialisation arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BaseComposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                                  \u001b[0;34m'Check the list of available parameters '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                                  \u001b[0;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                  (key, self))\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdelim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter cv for estimator Pipeline(memory=None,\n     steps=[('feats', FeatureUnion(n_jobs=None,\n       transformer_list=[('text', Pipeline(memory=None,\n     steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='c...alty='l2', random_state=None, solver='warn',\n          tol=0.0001, verbose=0, warm_start=False))]))]). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "pipe1.set_params(cv__min_df=6, \n",
    "                 cv__lowercase=False).fit(X_train, y_train)\n",
    "pred = pipe1.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import Counter\n",
    "\n",
    "class PosTagMatrix(BaseEstimator, TransformerMixin):\n",
    "    #normalise = True - devide all values by a total number of tags in the sentence\n",
    "    #tokenizer - take a custom tokenizer function\n",
    "    def __init__(self, tokenizer=lambda x: x.split(), normalize=True):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.normalize=normalize\n",
    "\n",
    "    #helper function to tokenize and count parts of speech\n",
    "    def pos_func(self, sentence):\n",
    "        return Counter(tag for word,tag in nltk.pos_tag(self.tokenizer(sentence)))\n",
    "\n",
    "    # fit() doesn't do anything, this is a transformer class\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    #all the work is done here\n",
    "    def transform(self, X):\n",
    "        X_tagged = X.apply(self.pos_func).apply(pd.Series).fillna(0)\n",
    "        X_tagged['n_tokens'] = X_tagged.apply(sum, axis=1)\n",
    "        if self.normalize:\n",
    "            X_tagged = X_tagged.divide(X_tagged['n_tokens'], axis=0)\n",
    "\n",
    "        return X_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CD', 'NNS', 'RB', 'NN', 'VBD', 'CC', 'VBG', 'IN', 'DT', 'PRP$', 'VBP',\n",
       "       'JJ', 'PRP', 'VBN', 'VBZ', 'TO', 'VB', 'MD', 'RP', 'WP', 'WRB', 'JJR',\n",
       "       'EX', 'RBR', 'JJS', 'WDT', 'PDT', 'RBS', 'NNP', 'FW', '$', 'UH', 'WP$',\n",
       "       'n_tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = PosTagMatrix()\n",
    "tranTag = obj.transform(X_train)\n",
    "tranTag.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testTag = obj.transform(X_test)\n",
    "tran.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x2 = test_x.loc[:, test_x.columns.isin(tranTag)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "pipe2 = Pipeline([\n",
    "    ('u1', FeatureUnion([\n",
    "        ('tfdif_features', Pipeline([\n",
    "            ('cv', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('pos_features', Pipeline([\n",
    "            ('pos', PosTagMatrix(tokenizer=nltk.word_tokenize) ),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('logit', LogisticRegression()),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(347,)\n",
      "(149,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 8634 features per sample; expecting 8635",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-76a355e6df3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpipe2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(classification_report(y_test, pred))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_final_estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 262\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 8634 features per sample; expecting 8635"
     ]
    }
   ],
   "source": [
    "pipe2.fit(X_train, y_train)\n",
    "pred = pipe2.predict(X_test)\n",
    "#print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
